{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "import numpy as np\n",
    "import ale_py\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import numpy as np\n",
    "import ale_py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards and penalties\n",
    "class LifePenaltyWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, death_penalty=-10, move_penalty= 0, moving_target_reward = -5, influence_reward=0, point_reward = 1):\n",
    "        super(LifePenaltyWrapper, self).__init__(env)\n",
    "        self.prev_lives = 0\n",
    "        self.death_penalty = death_penalty\n",
    "        self.move_penalty = move_penalty\n",
    "        self.influence_reward = influence_reward\n",
    "        self.moving_target_reward = moving_target_reward\n",
    "        self.point_reward = point_reward\n",
    "        self.last_action = None  # Track the last action taken by the agent\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        self.prev_lives = self.env.unwrapped.ale.lives()  # Ensure lives are synced\n",
    "        self.last_action = None  # Reset last action\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        self.last_action = action\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        if reward in [10, 20, 30, 40, 50]:\n",
    "            reward = self.point_reward\n",
    "        elif reward == 60:\n",
    "            reward = self.moving_target_reward\n",
    "\n",
    "        # Check if a life is lost and apply penalty\n",
    "        current_lives = self.env.unwrapped.ale.lives()\n",
    "        if current_lives < self.prev_lives:\n",
    "            reward = self.death_penalty  #when player dies the alien gives points for blowing up\n",
    "        self.prev_lives = current_lives\n",
    "\n",
    "        \n",
    "        if action in [2, 3]:  #  Penalize moving left or right\n",
    "            reward += self.move_penalty\n",
    "            \n",
    "\n",
    "        if action == 0:  # Penalize specific idle action\n",
    "            reward += 0\n",
    "\n",
    "        \n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed, stack_frames=4, mode=None):\n",
    "   \n",
    "    def _init():\n",
    "        env = gym.make(\"ALE/Galaxian-v5\", frameskip=1, render_mode=mode)  # Disable default frame skipping\n",
    "        \n",
    "        env = AtariPreprocessing(env, frame_skip=3)\n",
    "        env = LifePenaltyWrapper(env)\n",
    "        env = FrameStackObservation(env, stack_size=stack_frames)\n",
    "        \n",
    "\n",
    "        # Seed the environment\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        \n",
    "        return env\n",
    "    \n",
    "    return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class Custom3DCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super(Custom3DCNN, self).__init__(observation_space, features_dim)\n",
    "\n",
    "        frames, height, width = observation_space.shape\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=frames, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, frames, height, width)\n",
    "            n_flatten = self.cnn(sample_input).shape[1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.fc(self.cnn(observations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (4, 84, 84), uint8)\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import numpy as np\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Create and wrap the environment\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    lambda: make_env(seed=np.random.randint(20, 60))(), \n",
    "    n_envs=4\n",
    ")\n",
    "print(vec_env.observation_space)\n",
    "\n",
    "\n",
    "# Policy kwargs to use the custom CNN\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=Custom3DCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=256),  # Output dimension of the feature extractor\n",
    ")\n",
    "\n",
    "# Create the DQN model\n",
    "model = DQN(\n",
    "    \"CnnPolicy\",  # Use a CNN-based policy\n",
    "    vec_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=20000,\n",
    "    learning_starts=20000,\n",
    "    batch_size=128,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=(16, 'step'),\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.5,\n",
    "    exploration_final_eps=0.1,\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,   # Save every 10,000 steps\n",
    "    save_path='./checkpoints/',  # Directory to save the model\n",
    "    name_prefix='new_V1_train'  # Prefix for the checkpoint filenames\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=200000, callback=checkpoint_callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"new_v1_dqn_galaxian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    lambda: make_env(seed=np.random.randint(0, 5000))(), \n",
    "    n_envs=8\n",
    ")\n",
    "\n",
    "# Reload the saved model\n",
    "model = DQN.load(\n",
    "    \"new_v15_freq64_dqn_galaxian_long.zip\", \n",
    "    env=vec_env, \n",
    "    exploration_initial_eps=0.3,\n",
    "    exploration_final_eps=0.1, \n",
    "    exploration_fraction = 0.3, \n",
    "    learning_starts=20000, \n",
    "    train_freq=(128, 'step')\n",
    "    ) # it acts different wt smaller freq\n",
    "\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1000000, \n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"new\"\n",
    ")\n",
    "\n",
    "# Continue training\n",
    "model.learn(total_timesteps=1000000, callback=checkpoint_callback)\n",
    "\n",
    "# Save the retrained model\n",
    "model.save(\"new_v16_dqn_galaxian_long.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = DQN.load(\"new_v16_dqn_galaxian_long.zip\")\n",
    "\n",
    "# Create the environment with render_mode\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    lambda: make_env(seed=np.random.randint(0, 50), mode='human')(), \n",
    "    n_envs=1\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "obs = vec_env.reset()\n",
    "for _ in range(10000):\n",
    "    action, _states = model.predict(obs, deterministic=True)  # Use deterministic=True for evaluation\n",
    "    obs, reward, done, info = vec_env.step(action)  # Perform a step\n",
    "    print(f\"Reward: {reward}, info: {info}\")\n",
    "    vec_env.render()  # Optional: Visualize the gameplay\n",
    "    if done:\n",
    "        obs = vec_env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
